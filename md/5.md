# Произведение матриц 2

Вот мы сделали наше умножение матриц (нет), замерили FLOPS. Что-то маловато получается. 


На самом деле FLOPS считается как количество операций `MAD`/`FMA` в секунду. Эта операция делает `a * b + c`. Так как это 2 арифметических операции, `FLOPS = <количество MAD/FMA> * 2`.

FLOPS у нас маловато. Давайте чинить. На самом деле, у нас очень много времени проводится в ожидании доступа к памяти.

Давайте посчитаем еще количество байт, которое мы пишем в единицу времени. По идее, должно получиться в районе лимита пропускной способности, который будет написан в характеристиках видеокарты. Мы уперлись в память, а вычислительнную часть используем не полностью. Как это исправить?

> *Посмотрим на мой пример: [я пример с лекции 4](../examples/4) запускал на Intel UHD Graphics 620 (у тому же, в паре с не очень бодрой оперативкой LPDDR3-2133). Если верить её характеристикам, то она должна быть способна выдавать около 400 GFLOPS. На практике я получил же около 20 GFLOPS.*

Скажем, что нам надо умножить 2 столбца и 2 строчки, чтобы получить матричку 2x2. Тогда мы прочитаем каждый столбец и каждую строчку по 2 раза, то есть всего 4 обращения к памяти. Давайте закэшируем прочитанные столбцы и строчки в локальной памяти (она существенно быстрее глобальной памяти). Тогда у нас вместо 4 обращений к глобальной памяти будет всего 2.

Можно читать блоками, считать частичную сумму. Только вот синхронизировать это сложно. Но давайте делать. Сделаем локальные группы размера 8x8 (лучше завести под это `#define TILE 8`).


```c
#define TILE 8

kernel void mmul(global const float *ga, global const float *gb, global float *c, const uint N, const uint K, const uint M) {
    uint x = get_global_id(0); 
    uint y = get_global_id(1);

    uint lx = get_local_id(0);
    uint ly = get_local_id(1);

    local float la[TILE][TILE];
    local float lb[TILE][TILE];
    float sum = 0;
    for (uint k = 0; k < K; k += TILE) {
        if (k + lx < K && k + ly < K) {
            la[lx][ly] = ga[y * K + k + lx];
            lb[lx][ly] = gb[(k + ly) * N + x];
        }
        barrier(CLK_LOCAL_MEM_FENCE); 
        /*  
            Нам нужно убедиться, что все треды
            внутри локальной группы закончат
            все доступы к памяти до барьера,
            прежде чем начнут ходить в память
            после барьера. Барьеры есть типов
            CLK_LOCAL_MEM_FENCE и CLK_GLOBAL_MEM_FENCE.
            БАРЬЕРЫ СИНХРОНИЗИРУЮТ ТОЛЬКО ТРЕДЫ
            ВНУТРИ ОДНОЙ ЛОКАЛЬНОЙ ГРУППЫ. МЕЖДУ
            ЛОКАЛЬНЫМИ ГРУППАМИ СИНХРОНИЗАЦИИ НЕТ.
        */ 
        for (uint z = 0; z < TILE; z++) {
            sum += la[z][ly] * lb[lx][z];
        }
        barrier(CLK_LOCAL_MEM_FENCE);
    }

    c[y * N + x] = sum;
}
```

> Тут был срач про замер скорости в домашке (опять), но ноль информации о том, как ее ускорять. Зал просит замеры скорости, Скаков говорит, почему получится плохо.

Если доказать компилятору, что у вас размер локальной группы будет равен размеру одного warp, то он может соптимизировать барьеры. Но для правильности мы должны барьеры писать всегда.

На хосте теперь будем запускаться, явно указывая размер локальной группы 8x8. В OpenCL 1.0 размер глобальной работы обязан нацело делиться на размер локальной работы. В более поздних версиях можно без деления нацело, но не рекомендуется. Поэтому если матрицы кратные, то все хорошо. Если матрицы не кратные, то не всё хорошо. Как это фиксить?
*   Можно увеличить размер матриц до кратного локальной группе и заполнить лишнее нулями.
*   Можно внутри kernel'а поставить `if` при записи в `la` и `lb` из `ga` и `gb`, который ставит 0, если мы вышли за границу матрицы.

    **Но осторожнее с барьерами!** `if` надо втыкать так, чтобы все все равно все треды доходили до барьеров, иначе все повиснет!

К локальной памяти тоже надо обращаться осторожно, если мы хотим быть быстрыми.

Возможно, множество жирных обращений тоже стоит оптимизировать. Давайте читать большими кусками. 

---

Локальная память состоит из банков памяти. Как правило, банки памяти чередуются через 4 байта (0, 1, 2, 3, 0, 1, ...). Чтение производится из 4 банков одновременно. Поэтому расставляйте чтения аккуратно!

Если одновременно несколько чтений из одной ячейки, то это по времени как одно чтение.

Можно посмотреть ассемблер видеокарты, который у нас сгенерировался. Для разных производителей видеокарт есть разные команды.